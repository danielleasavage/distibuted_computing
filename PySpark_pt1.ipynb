{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Part 1\n",
    "Notebook Created by Danielle Savage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining PySpark & Its Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributed Computing**\n",
    "\n",
    "Let us pose a problem. You have large amounts of data, so much that it cannot fit on the machines that you currently have? Do you buy bigger machines in the hopes that you will be able to store it all in one place, or do you distribute different datasets across different machines? \n",
    "\n",
    "Now that this data is no longer in one place how do you work on projects that need data that has been distributed this way? \n",
    "\n",
    "The simple anwser is Spark!\n",
    "\n",
    "**What is Apache Spark?**\n",
    "\n",
    "A cluster computing framework that uses RDDs for efficient, fault tolerant data sharing. \n",
    "\n",
    "**What is an RDD?**\n",
    "\n",
    "A Resilient Distributed Dataset. They are 1) Distributed into partions 2) Immutable, meaning they are read-only 3) Resilient, RDDs track lineage and are therfore easily rebuilt in the case of system faliure of data loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.231.149.111:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this is to ensure that the SparkContext was loaded properly when launching Jupyter\n",
    "sc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an RDD via external data\n",
    "The first of two ways to load data in to an RDD is to do so via an external file. For the purpose of this example I will load in the README for this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('README.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an RDD via and array or list\n",
    "The second way is to give spark a collection such as a list like the one I created below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize(['this is ', 'an input', 'via a python list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with RDDs\n",
    "\n",
    "While loading in data is important, likely we are loading in the data with a purpose, either to analyze it or at least to view in in some way. This section with layout what you are able to do in Spark, and set the foundation for the next notebook.\n",
    "\n",
    "** RDD Operations**\n",
    "\n",
    "Transformations - Remember that RDDs are immutable, so when transforming them, Spark creates a new RDD by preforming some sort of data manipulation on a previous RDD.\n",
    "\n",
    "Actions - RDDs have what is called Lazy Evaluation, or Call By Need. This simply means that computation does not take place until required. In the case of Spark Computation is note required until and Action is triggered. *collect* is one specific action that will force computation and return the RDD resulting from previous transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is ', 'an input', 'via a python list']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect Example - The RDDs cannot be viewed until collect is called\n",
    "lines.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
