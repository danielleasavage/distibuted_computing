{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML & SparkSQL\n",
    "Notebook Created By Danielle Savage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.233.23.171:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets begin with checking our Spark Context\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option('uri','mongodb://54.191.76.32/msan697.item_ids').load()\n",
    "\n",
    "players = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option('uri','mongodb://54.191.76.32/msan697.players').load()\n",
    "\n",
    "match = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option('uri','mongodb://54.191.76.32/msan697.match').load()\n",
    "\n",
    "purchases = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option('uri','mongodb://54.191.76.32/msan697.purchase_log').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanatory Analysis - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlContext = SQLContext(sc)\n",
    "\n",
    "players_new = players.select(['match_id','player_slot','kills','deaths','assists','gold_per_min','xp_per_min'])\n",
    "\n",
    "match_new = match.select(['match_id','duration', 'radiant_win'])\n",
    "\n",
    "players_match = players_new.join(how='left_outer', on='match_id', other=match_new)\n",
    "\n",
    "purch_new = purchases.select(['player_slot','match_id','item_id','time'])\n",
    "\n",
    "pmatch_purch = purch_new.join(how='left_outer', on=['match_id', 'player_slot'], other=players_match)\n",
    "\n",
    "pmatch_purch.groupBy('match_id').count().agg({'count':'avg'}).show()\n",
    "\n",
    "pmatch_purch.groupBy('match_id').count().agg({'count':'min'}).show()\n",
    "\n",
    "pmatch_purch.groupBy('match_id').count().agg({'count':'max'}).show()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "def make_win_col(pslot, radwin):\n",
    "    if (pslot < 50) ^ (radwin == 'False'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "myAtomicUDF = udf(make_win_col, IntegerType())\n",
    "\n",
    "pmpurch = pmatch_purch.withColumn('win', myAtomicUDF(pmatch_purch.player_slot, pmatch_purch.radiant_win))\n",
    "\n",
    "final_pmpurch= pmpurch.drop('radiant_win','player_slot','match_id')\n",
    "\n",
    "print final_pmpurch.count()\n",
    "\n",
    "onehotenc = OneHotEncoder(inputCol='item_id', outputCol=\"itemid-onehot\", dropLast=False)\n",
    "one_hot_df = onehotenc.transform(final_pmpurch).drop('item_id')\n",
    "one_hot_df = one_hot_df.withColumnRenamed(\"itemid-onehot\", 'item_id')\n",
    "\n",
    "va = VectorAssembler(outputCol='features', inputCols=sorted(one_hot_df.columns)[0:-1])\n",
    "explanatory_df = va.transform(one_hot_df).select('features', one_hot_df['win'].alias('label'))\n",
    "\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=1000, fitIntercept=True)\n",
    "lrmodel = lr.fit(explanatory_df)\n",
    "\n",
    "print lrmodel.coefficients\n",
    "print lrmodel.intercept\n",
    "\n",
    "############################################\n",
    "###### ATTEMPT TO DO A RANDOM FOREST ######\n",
    "#####    FAILED DUE TO NUMBER OF     ######\n",
    "#####       RESPONSE CATAGORIES   #########\n",
    "###########################################\n",
    "sqlContext.clearCache()\n",
    "\n",
    "def safeFloat(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return str(x)\n",
    "\n",
    "def preprocess(f):\n",
    "    # takes a file name w/ header row and returns a DF\n",
    "    pre_data = sc.textFile(f).map(lambda x:x.split(','))\n",
    "    head = pre_data.first()\n",
    "    body = pre_data.filter(lambda x:x[0] != head[0]).map(lambda x:[-1.0 if len(xi)==0 or xi=='null' else safeFloat(xi) for xi in x])\n",
    "    return body.toDF(head)\n",
    "\n",
    "playersS3=preprocess('s3n://dota-ds-jr-ss-fh/players.csv')\n",
    "\n",
    "players_for_preds = playersS3.drop('account_id','unit_order_patrol', 'unit_order_radar','unit_order_vector_target_position', 'unit_order_set_item_combine_lock', 'unit_order_continue', 'unit_order_cast_rune', 'unit_order_move_to_direction', 'unit_order_eject_item_from_stash', 'unit_order_taunt', 'unit_order_cast_toggle_auto', 'unit_order_disassemble_item', 'unit_order_none', 'gold_abandon').withColumn('hero_ids',1*playersS3.hero_id).drop('hero_id')\n",
    "\n",
    "final_hero_df = players_for_preds.select('kills', 'deaths', 'assists', 'denies','last_hits', 'xp_per_min', 'gold_per_min', 'gold_spent', 'gold','hero_damage','tower_damage','item_0','item_1','item_2','item_3','item_4','item_5','level','hero_ids')\n",
    "\n",
    "va2 = VectorAssembler(outputCol='features', inputCols=final_hero_df.columns[:-1])\n",
    "predictive_df2 = va2.transform(final_hero_df).select('features', final_hero_df['hero_ids'].alias('label'))\n",
    "\n",
    "splits = predictive_df2.randomSplit([0.9, 0.1])\n",
    "split2 = splits[0].randomSplit([0.8, 0.2])\n",
    "train = splits[0].cache()\n",
    "valid = split2[1].cache()\n",
    "test = splits[1].cache()\n",
    "\n",
    "\n",
    "#rf = RandomForestClassifier(labelCol='label', featuresCol='features')\n",
    "#hero_model = rf.fit(train)\n",
    "\n",
    "#prediction = hero_model.transform(valid)\n",
    "#evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "#hero_f1 = evaluator.evaluate(prediction)\n",
    "\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
